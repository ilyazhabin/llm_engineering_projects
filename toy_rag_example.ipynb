{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60151b04-7ba4-4b14-9385-63dd4ad43338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import fitz  \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87117c75-cf7a-49a2-8b3b-1ba2b97cf72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FinancialRAG:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the Financial RAG system with:\n",
    "        - Anthropic API client\n",
    "        - Sentence Transformer for embeddings\n",
    "        - ChromaDB for vector storage\n",
    "        \"\"\"\n",
    "        api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "        if not api_key:\n",
    "            raise ValueError(\"ANTHROPIC_API_KEY not found in environment variables\")\n",
    "            \n",
    "        self.anthropic_client = anthropic.Anthropic(api_key=api_key)\n",
    "        self.embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\") # Lightweight model \n",
    "        \n",
    "        # Simple in-memory vector store using ChromaDB\n",
    "        self.chroma_client = chromadb.Client()\n",
    "        self.collection = self.chroma_client.create_collection(\n",
    "            name=\"financial_docs\",\n",
    "            metadata={\"hnsw:space\": \"cosine\"} # Cosine similarity for text\n",
    "        )\n",
    "        \n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract and clean text from each page of a PDF document.\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: Path to the PDF file\n",
    "            \n",
    "        Returns:\n",
    "            List of page dictionaries with:\n",
    "            - content: Cleaned text content\n",
    "            - page_num: Page number (1-indexed)\n",
    "            - source: PDF filename\n",
    "        \"\"\"\n",
    "        doc = fitz.open(pdf_path)\n",
    "        pages = []\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            text = page.get_text()\n",
    "            \n",
    "            # Clean up text\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            \n",
    "            if len(text) > 100:  # Only keep pages with substantial content\n",
    "                pages.append({\n",
    "                    'content': text,\n",
    "                    'page_num': page_num + 1,\n",
    "                    'source': Path(pdf_path).name\n",
    "                })\n",
    "        \n",
    "        doc.close()\n",
    "        logger.info(f\"Extracted {len(pages)} pages from {pdf_path}\")\n",
    "        return pages\n",
    "    \n",
    "    def create_chunks(self, pages: List[Dict], chunk_size: int = 1000) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Split page content into manageable chunks for embedding.\n",
    "        \n",
    "        Args:\n",
    "            pages: List of page dictionaries from extract_text_from_pdf\n",
    "            chunk_size: Target number of words per chunk\n",
    "            \n",
    "        Returns:\n",
    "            List of chunk dictionaries with:\n",
    "            - id: Unique chunk identifier\n",
    "            - content: Chunk text\n",
    "            - source/page_num: Reference back to original document\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        for page in pages:\n",
    "            text = page['content']\n",
    "            words = text.split()\n",
    "            \n",
    "            # Split into chunks\n",
    "            for i in range(0, len(words), chunk_size):\n",
    "                chunk_words = words[i:i + chunk_size]\n",
    "                chunk_text = ' '.join(chunk_words)\n",
    "                \n",
    "                if len(chunk_text) > 50:  # Skip tiny chunks\n",
    "                    chunk_id = f\"{page['source']}_page{page['page_num']}_chunk{i//chunk_size}\"\n",
    "                    \n",
    "                    chunks.append({\n",
    "                        'id': chunk_id,\n",
    "                        'content': chunk_text,\n",
    "                        'source': page['source'],\n",
    "                        'page_num': page['page_num']\n",
    "                    })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def add_documents(self, pdf_paths: List[str]):\n",
    "        \"\"\"\n",
    "        Process multiple PDFs and add them to the vector database.\n",
    "        \n",
    "        Steps:\n",
    "        1. Extract text from each PDF\n",
    "        2. Chunk the content\n",
    "        3. Generate embeddings\n",
    "        4. Store in ChromaDB with metadata\n",
    "        \n",
    "        Args:\n",
    "            pdf_paths: List of paths to PDF files\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        \n",
    "        for pdf_path in pdf_paths:\n",
    "            logger.info(f\"Processing {pdf_path}\")\n",
    "            \n",
    "            # Extract pages\n",
    "            pages = self.extract_text_from_pdf(pdf_path)\n",
    "            \n",
    "            # Create chunks\n",
    "            chunks = self.create_chunks(pages)\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        if not all_chunks:\n",
    "            logger.warning(\"No content found in documents\")\n",
    "            return\n",
    "        \n",
    "        # Create embeddings\n",
    "        logger.info(\"Creating embeddings...\")\n",
    "        texts = [chunk['content'] for chunk in all_chunks]\n",
    "        embeddings = self.embedding_model.encode(texts, show_progress_bar=True)\n",
    "        \n",
    "        # Add to vector database\n",
    "        self.collection.add(\n",
    "            embeddings=embeddings.tolist(),\n",
    "            documents=texts,\n",
    "            metadatas=[{\n",
    "                'source': chunk['source'], \n",
    "                'page_num': chunk['page_num']\n",
    "            } for chunk in all_chunks],\n",
    "            ids=[chunk['id'] for chunk in all_chunks]\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Added {len(all_chunks)} chunks to database\")\n",
    "    \n",
    "    def search(self, question: str, n_results: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Semantic search for relevant document chunks.\n",
    "        \n",
    "        Args:\n",
    "            question: Natural language question\n",
    "            n_results: Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of relevant chunks with:\n",
    "            - content: Matching text\n",
    "            - source/page_num: Document reference\n",
    "            - relevance_score: 1 - cosine distance\n",
    "        \"\"\"\n",
    "        question_embedding = self.embedding_model.encode([question])\n",
    "        \n",
    "        results = self.collection.query(\n",
    "            query_embeddings=question_embedding.tolist(),\n",
    "            n_results=n_results,\n",
    "            include=['documents', 'metadatas', 'distances']\n",
    "        )\n",
    "        \n",
    "        relevant_chunks = []\n",
    "        for doc, meta, distance in zip(\n",
    "            results['documents'][0], \n",
    "            results['metadatas'][0], \n",
    "            results['distances'][0]\n",
    "        ):\n",
    "            relevant_chunks.append({\n",
    "                'content': doc,\n",
    "                'source': meta['source'],\n",
    "                'page_num': meta['page_num'],\n",
    "                'relevance_score': 1 - distance  # Higher is better\n",
    "            })\n",
    "        \n",
    "        return relevant_chunks\n",
    "    \n",
    "    def answer_question(self, question: str) -> Dict:\n",
    "        \"\"\"\n",
    "        End-to-end question answering pipeline.\n",
    "        \n",
    "        1. Perform semantic search for relevant context\n",
    "        2. Construct prompt with context\n",
    "        3. Query Claude model\n",
    "        4. Format response with sources\n",
    "        \n",
    "        Args:\n",
    "            question: Financial question to answer\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with:\n",
    "            - answer: Formatted response\n",
    "            - sources: Top document references\n",
    "            - question: Echo of original question\n",
    "        \"\"\"\n",
    "        \n",
    "        # Find relevant information\n",
    "        relevant_chunks = self.search(question, n_results=6)\n",
    "        \n",
    "        if not relevant_chunks:\n",
    "            return {\n",
    "                'answer': \"No relevant information found.\",\n",
    "                'sources': []\n",
    "            }\n",
    "        \n",
    "        # Prepare context\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[From {chunk['source']}, Page {chunk['page_num']}]:\\n{chunk['content']}\"\n",
    "            for chunk in relevant_chunks\n",
    "        ])\n",
    "        \n",
    "        # Claude prompt\n",
    "        prompt = f\"\"\"You are analyzing financial statements. Use the provided excerpts to answer the question.\n",
    "\n",
    "FINANCIAL STATEMENT EXCERPTS:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Instructions:\n",
    "- Give specific numbers when comparing periods\n",
    "- Calculate percentage changes when relevant  \n",
    "- Always mention which document and page your information comes from\n",
    "- If you can't find the exact information needed, say so clearly: \"I don't know\"\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.anthropic_client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",\n",
    "                max_tokens=1000,\n",
    "                temperature=0.1,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            \n",
    "            answer = response.content[0].text\n",
    "            \n",
    "        except Exception as e:\n",
    "            answer = f\"Error getting answer: {str(e)}\"\n",
    "        \n",
    "        # Prepare response with sources\n",
    "        sources = []\n",
    "        for chunk in relevant_chunks[:3]:  # Top 3 \n",
    "            sources.append({\n",
    "                'document': chunk['source'],\n",
    "                'page': chunk['page_num'],\n",
    "                'relevance': f\"{chunk['relevance_score']:.2f}\"\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'question': question\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ac39b7a-6de7-4157-963f-44a486a1244e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:__main__:Processing knowledge-base/gazprom-ifrs-2024-12mnth-en.pdf\n",
      "INFO:__main__:Extracted 57 pages from knowledge-base/gazprom-ifrs-2024-12mnth-en.pdf\n",
      "INFO:__main__:Creating embeddings...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading financial statements...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e39b26bb07e44e6a29499304f77313f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Added 57 chunks to database\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Q: How much did property, plant and equipment change between periods?\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00a44d4537d423298da70a302348815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Based on the financial statement excerpts from gazprom-ifrs-2024-12mnth-en.pdf (Page 29), I can track the net book value of total Property, Plant and Equipment across three periods:\n",
      "\n",
      "31 December 2022: 17,419,060 million rubles\n",
      "31 December 2023: 18,436,207 million rubles\n",
      "31 December 2024: 20,779,950 million rubles\n",
      "\n",
      "Changes between periods:\n",
      "- From 2022 to 2023: Increased by 1,017,147 million rubles (+5.8%)\n",
      "- From 2023 to 2024: Increased by 2,343,743 million rubles (+12.7%)\n",
      "- Total change from 2022 to 2024: Increased by 3,360,890 million rubles (+19.3%)\n",
      "\n",
      "The largest year-over-year increase was between 2023 and 2024, showing significant growth in the company's property, plant and equipment assets.\n",
      "\n",
      "Sources:\n",
      "- gazprom-ifrs-2024-12mnth-en.pdf, Page 29\n",
      "- gazprom-ifrs-2024-12mnth-en.pdf, Page 38\n",
      "- gazprom-ifrs-2024-12mnth-en.pdf, Page 31\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    rag = FinancialRAG()\n",
    "    \n",
    "    pdf_files = [\n",
    "        'knowledge-base/gazprom-ifrs-2024-12mnth-en.pdf'\n",
    "    ]\n",
    "    \n",
    "    print(\"Loading financial statements...\")\n",
    "    rag.add_documents(pdf_files)\n",
    "    \n",
    "    questions = [\n",
    "        \"How much did property, plant and equipment change between periods?\"\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        print(f\"\\n\" + \"=\"*50)\n",
    "        print(f\"Q: {question}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        result = rag.answer_question(question)\n",
    "        \n",
    "        print(f\"Answer: {result['answer']}\")\n",
    "        print(f\"\\nSources:\")\n",
    "        for source in result['sources']:\n",
    "            print(f\"- {source['document']}, Page {source['page']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9aa0aa43-90a2-4afb-9a5d-d867fa22cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client = chromadb.Client()\n",
    "try:\n",
    "    client.delete_collection(\"financial_docs\")\n",
    "except:\n",
    "    pass  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b83878d-df0c-4421-bc34-fc4bd61fe88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing knowledge-base/gazprom-ifrs-2024-12mnth-en.pdf\n",
      "INFO:__main__:Extracted 57 pages from knowledge-base/gazprom-ifrs-2024-12mnth-en.pdf\n",
      "INFO:__main__:Creating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0024c2fa533d4633ba648c39b4d27cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Added 57 chunks to database\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4d84b14a91456d9ebe6259d246f33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the financial statement excerpts, I'll analyze the major factors impacting net profit:\n",
      "\n",
      "From page 45 (Note 30 \"Net Cash from Operating Activities\"), we can see several key indicators and their changes:\n",
      "\n",
      "1. Depreciation: RUB 1,377,774 million in 2024 vs RUB 982,058 million in 2023 (+40% increase)\n",
      "\n",
      "2. Net finance expense: RUB 35,462 million in 2024 vs RUB 649,745 million in 2023 (significant decrease of 95%)\n",
      "\n",
      "3. Impairment loss on assets and change in provision for post-employment benefits: RUB 516,519 million in 2024 vs RUB 1,490,124 million in 2023 (65% decrease)\n",
      "\n",
      "4. Share of profit of associates and joint ventures: RUB 242,008 million in 2024 vs RUB 354,364 million in 2023 (32% decrease)\n",
      "\n",
      "Looking at the magnitude of these changes, the most significant impact on net profit came from the reduction in impairment loss on assets and change in provision for post-employment benefits, which decreased by RUB 973,605 million (from RUB 1,490,124 million to RUB 516,519 million). This represents the largest absolute change among all the major indicators affecting profit.\n",
      "\n",
      "The second most significant impact came from the reduction in net finance expense, which decreased by RUB 614,283 million.\n",
      "\n",
      "These improvements helped move the company from a loss before tax of RUB 659,070 million in 2023 to a profit before tax of RUB 1,662,936 million in 2024.\n"
     ]
    }
   ],
   "source": [
    "rag = FinancialRAG()\n",
    "rag.add_documents(['knowledge-base/gazprom-ifrs-2024-12mnth-en.pdf'])\n",
    "result = rag.answer_question(\"Which indicator had the greatest impact on the company's net profit?\")\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b7236-2e3b-4f9e-bb0a-a3c387e88ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
